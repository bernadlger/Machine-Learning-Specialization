{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3247447-9124-4d2b-a9ce-5f52e88ef04b",
   "metadata": {},
   "source": [
    "Goals:  \n",
    "Extend our regression model routines to support multiple features   \n",
    "Rewrite prediction, cost and gradient routines to support multiple features   \n",
    "Utilize NumPy np.dot to vectorize their implementations for speed and simplicity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee366f3e-e516-4f5d-9f21-8559b3b35fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aca5d-c267-4916-972b-bf442e447bb5",
   "metadata": {},
   "source": [
    "\n",
    " # Problem Statement\n",
    "\n",
    "We will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "We will build a linear regression model using these values so we can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4171c7c6-ba46-4308-8833-4d9aaf1c35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = np.array([[2104, 5, 1, 45],[1416, 3, 2, 40],[852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e05ca-67d1-42c0-8e36-2db5f0ca7937",
   "metadata": {},
   "source": [
    "\n",
    "## Matrix X containing our examples\n",
    "Examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "m=satır  \n",
    "n=sütun  \n",
    "X[satır,sütun] ----> rows indicate examples(such as houses in our example) and columns indicates features(such as size, age,..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ce64a-9a90-4489-9e50-7fe1a727866c",
   "metadata": {},
   "source": [
    "\n",
    "##  Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "\n",
    "\n",
    "\n",
    "* $b$ is a scalar parameter.\n",
    "\n",
    "* Note: For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8490fb72-f951-4e3e-86a3-ec30df8a8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fa79d-5a19-4832-917f-b5b549b656e7",
   "metadata": {},
   "source": [
    "\n",
    "# Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba39a9d-e8ed-4f36-a916-a9a887688c23",
   "metadata": {},
   "source": [
    " \n",
    "The dot product multiplies the values in two vectors element-wise and then sums the result.   \n",
    "Vector dot product requires the dimensions of the two vectors to be same.   \n",
    "The dot product is expected to return a scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df0454-a234-459d-b2ca-609801429bee",
   "metadata": {},
   "source": [
    "Note: The NumPy dot function is able to use parallel hardware in your computer. The ability of the NumPy dot function to use\n",
    "parallel hardware makes it much more efficient than the for loop or the sequential calculation. If you have very large training sets,\n",
    "vectorized implementation will make a huge difference in the running time of your learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933c99e-ed89-432f-be92-7821e8380925",
   "metadata": {},
   "source": [
    "\n",
    "## Using a for loop to make a single prediction\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78767900-aad2-455b-82a4-70cb7e44324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_loop(w,b,x):\n",
    "    n = x.shape[0]\n",
    "    p=0\n",
    "    for i in range(n):\n",
    "        p_i = w[i] * x[i]\n",
    "        p = p + p_i\n",
    "    p = p + b\n",
    "    return p \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab072faf-27ff-4bde-a965-baac1f61a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our treaining data \n",
    "x_vec = X_Train[0,:] #İlk satırı ve tüm sütunları seçtik. (Slicing). Yani ilk evin özelliklerini aldık\n",
    "#make a prediction\n",
    "f_wb = prediction_loop(w_init, b_init, x_vec)\n",
    "print(f\" Prediction: {f_wb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f79455-1b8e-43a1-817b-7fa0ca878ab6",
   "metadata": {},
   "source": [
    "\n",
    "## Using vectorization to make a single prediction\n",
    "\n",
    " We can make use of vector operations to speed up predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()` can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "538da3aa-732a-44ca-b743-2d76313ce375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_vectorized(w,b,x):\n",
    "    p = np.dot(w,x) + b \n",
    "    return p\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cfee68d-bca9-45b4-bcc0-fbced0f6e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 459.99999761940825\n"
     ]
    }
   ],
   "source": [
    "x_vec = X_Train[0,:]\n",
    "f_wb = prediction_vectorized(w_init, b_init, x_vec)\n",
    "print(f\"Prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be2fac-852c-4e2c-924a-b99de6f53b64",
   "metadata": {},
   "source": [
    "\n",
    "# Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features!! (there is dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f363c00-6c10-4d48-89e0-80472829158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(w,b,x,y):\n",
    "    m = x.shape[0] #bu bizim satır yani örnek sayımızı gösteriyor\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i],w) + b \n",
    "        cost = cost + (f_wb_i - y[i])**2\n",
    "    cost = cost / (2*m)\n",
    "    return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47f4416b-c360-4823-8a0c-222fde410d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cost at optimal w : 1.5578904880036537e-12\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(w_init, b_init, X_Train, y_train)\n",
    "print (f\" Cost at optimal w : {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c3138-36fe-451b-a2a6-a2056a1529d6",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e73e677-4f46-4c2b-b407-09e4fb8ec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(w,b,x,y):\n",
    "    m,n = x.shape #number of examples, number of features\n",
    "    dj_dw = np.zeros((n)) #feature sayısı kadar w olacağı için 1 boyutlu n elemanlı dizi oluşturduk (w0,w1,w2,w3)\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range (m): #Bu dış döngü veri setindeki her bir evi işleyecek\n",
    "        err = ((np.dot(x[i],w) + b ) - y[i])\n",
    "        for j in range (n): # bu döngü w0,w1,..wn bulmamız için \n",
    "            dj_dw[j] = dj_dw[j] + (err * x[i,j])\n",
    "        dj_db = dj_db + err\n",
    "        \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m \n",
    "\n",
    "    return dj_dw, dj_db\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a6a2c8f-6c62-44f5-bae1-6061b9d0c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_in,b_in,x,y,cost_function, gradient_function, alpha, num_iters):\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range (num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(w,b,x,y)\n",
    "        #Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "         # Save cost J at each iteration\n",
    "        if i < 100000: # prevent resource exhaustion\n",
    "            J_history.append(cost_function(w,b,x,y))\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\" Iteration: {i:4d} : Cost {J_history[-1]:8.2f} \")\n",
    "    return w, b, J_history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea9342-f4c6-4a3a-848b-1f2298394c40",
   "metadata": {},
   "source": [
    "In the next cell you will test the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7bc2366a-e431-4dc5-a6ea-dc3ee456598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration:    0 : Cost  2529.46 \n",
      " Iteration:  100 : Cost   695.99 \n",
      " Iteration:  200 : Cost   694.92 \n",
      " Iteration:  300 : Cost   693.86 \n",
      " Iteration:  400 : Cost   692.81 \n",
      " Iteration:  500 : Cost   691.77 \n",
      " Iteration:  600 : Cost   690.73 \n",
      " Iteration:  700 : Cost   689.71 \n",
      " Iteration:  800 : Cost   688.70 \n",
      " Iteration:  900 : Cost   687.69 \n",
      " w, b found by gradient descent: [ 0.2   0.   -0.01 -0.07], -0.00\n",
      " Prediction:  426.19, target value: 460\n",
      " Prediction:  286.17, target value: 232\n",
      " Prediction:  171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init) #w_init ile aynı şekle sahip, ama tüm elemanları sıfır olan bir dizi olarak oluşturuluyor.\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent\n",
    "w_final , b_final, J_hist =  gradient_descent(initial_w, initial_b, X_Train, y_train, compute_cost, compute_gradient, alpha, iterations)\n",
    "print(f\" w, b found by gradient descent: {np.round(w_final,2)}, {b_final:0.2f}\")\n",
    "m,_ = X_Train.shape\n",
    "for i in range (m):\n",
    "    print(f\" Prediction: {np.dot(X_Train[i],w_final) + b_final : 0.2f}, target value: {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebed9f0-d484-4df0-af87-d210c673ebbf",
   "metadata": {},
   "source": [
    "These results are not inspiring! Cost is still declining and our predictions are not very accurate. The next lab we will explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2ce99-f5e2-4b33-ad46-a274074a95f8",
   "metadata": {},
   "source": [
    "In this lab we:\n",
    "- Redeveloped the routines for linear regression, now with multiple variables.\n",
    "- Utilized NumPy `np.dot` to vectorize the implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db044db9-18ed-4a7b-82c4-e75a8405571d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
